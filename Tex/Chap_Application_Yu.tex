\makeatletter
\newenvironment{breakablealgorithm}
  {% \begin{breakablealgorithm}
  \begin{flushleft}
    \refstepcounter{algorithm}% New algorithm
    \hrule height.8pt depth0pt \kern2pt% \@fs@pre for \@fs@ruled
    \renewcommand{\caption}[2][\relax]{% Make a new \caption
      {\raggedright\textbf{\ALG@name~\thealgorithm} ##2\par}%
      \ifx\relax##1\relax % #1 is \relax
        \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##2}%
      \else % #1 is not \relax
        \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##1}%
      \fi
      \kern2pt\hrule\kern2pt
    }
  }{% \end{breakablealgorithm}
    \kern2pt\hrule\relax% \@fs@post for \@fs@ruled
  \end{flushleft}
  }
\makeatother


\section{COStream数据流编程语言及其在深度学习中的应用（于俊清）}
\subsection{引言}
近些年，多/众核架构已经被普遍认为是开发并行性的有效平台，一方面它为各类应用提供了强大的并行计算能力，特别是在数字媒体处理和科学计算等计算密集型的应用领域；另一方面它也将如何充分挖掘程序的并行性以及如何充分利用资源等问题暴露给了编程人员。传统的编程模型，如C、C++和Fortran，主要对应的都是单指令流和集中存储管理模式，无法很好的适合这种特殊的并行体系结构。当前比较流行的编程模型，如OpenMP和MPI，要求程序员必须熟悉并行系统的底层结构，编程人员设计并行程序时需要根据系统底层结构进行精心的任务划分、数据通信以及同步设计。导致程序性能受制于编程人员并行算法的设计和对并行系统的理解，增加了编程人员，尤其是各个应用领域编程人员的编程复杂度。

基于多核与分布式系统结构以及数据流编程模型的特点，华中科技大学的智能媒体计算与网络安全实验室设计并实现了一种面向数据流编程的编程语言与编译系统 -- COStream\citep{张维维2013COStream,WeiStreamTMC,weiCompile}。语言的名称由3个关键字：Composite、Operator和Stream组合而来。COStream程序采用有向图来描述应用的处理过程，图中节点表示计算，边表示数据依赖，边的方向表示数据流动方向。



\subsection{COStream 数据流编程语言}
COStream是在C语言文法基础上加入了表征数据流图结构文法而形成的数据流编程语言，它实现了对数据流图最基本的抽象，方便数据流程序的编写。

\subsubsection{与 C 语言的关系}
（1）COStream 保留了 C 语言的全局变量声明， 在文件头部声明了全局变量名后, 可以在后续的函数，Composite 和 Operator 中使用。下面给出了一些示例代码:
\begin{algorithm}\label{algo:standard}
int i = 0; double j = 0.0; //支持整数和浮点数\\
int i = 1, j = 2, k = 3; //支持一条语句中声明多个变量\\
int i = 1e10, j = 0x16; //初始值支持科学记数法和16进制\\
int a[3] = { 1,2,3 };   //支持数组声明和赋初值
\end{algorithm}

（2）COStream 支持全局函数声明，但由于其是数据流编程语言，而非面向对象编程语言，不能通过 this 关键字来访问执行上下文， 因此全局函数多用来做一些数据计算的复用，下面给出一些示例：

\begin{algorithm}\label{algo:standard}
int sum(int a, int b)\{\\
\hspace*{1 pc} return a+b; // 最基础的加法运算\\
\}\\
double expr(double base, double x)\{\\
\hspace*{1 pc} return base**x; // 支持使用两个星号**来表示 base 的 x 次方\\
\}
\end{algorithm}
（3）COStream 支持的基础运算类型有: 
\begin{itemize}
    \item 双目运算符: + 加, - 减, * 乘, / 除, \% 取余, ** 乘方
    \item 位运算符: \& 按位与, | 按位或, \^{} 异或, << 左移, >> 右移
    \item 逻辑运算符: < > <= >= == != \&\& || !
    \item 单目运算符: + 正号, - 负号, ++ 自增, -- 自减
    \item 三目运算符: ? :
    \item 成员运算符: . 例如 S.a
\end{itemize}

但与 C 语言不同的是, COStream 抛弃了 C 语言中函数必须先声明后调用的限制, 而参考其它语言的文法, 规定函数的调用可以放在声明前, 实现方式为通过编译器在文法解析时的预处理, 优先将全局变量和函数声明信息存入符号表, 使得解析函数调用时可以直接从符号表中提取相关信息, 而非依赖上下文环境。

\subsubsection{Stream}
数据流（stream）是由一系列数据流成员（token）组成的序列，作为通信载体连接数据流图中各个计算单元（actor），它是对数据流图中通信边的抽象，为计算单元提供可并行操作的数据对象。组成stream的token是一种复合数据类型，运算主要是对token进行的。存储器中对数据流的安排对于程序员而言是透明的。COSteam代表token的stream声明类似于C语言的结构体，可以包括任意基本C数据类型（如int、float和double等）、字符串类型（string）和基本类型的数组等。数据流分为输入数据流和输出数据流两种类型。在SDF图中输入数据流对应actor的输入边，对actor是只读的；输出数据流对应actor的输出边，对actor是可读写的。一般来说，一个stream是一个actor的输入数据流同时又是另一个actor的输出数据流。

\subsubsection{Operator}

同步数据流图中最基本的组成单元和计算单元actor在COStream中由operator文法结构表示，operator在SDF中被抽象成一个结点，专门用来处理stream中的数据。operator定义了actor输入流、输出流和具体的处理过程。operator由头部定义和体定义组成，其中operator头部定义了该operator处理的输入、输出流以及operator名称，COStream暂时不支持匿名operator的定义。COStream中一个operator可以有多个输入流和多个输出流。Operator体包括declare(静态变量声明)、init(静态变量初始化)、work和window四个部分。其中，work函数是operator内最细粒度的运算，是operator的核心结构，是数据流程序迭代执行过程中每次执行计算的单元。对operator输入和输出缓冲区的访问操作也是在work中进行的。operator内部变量的声明部分(declare)主要是为了定义该operator的work在执行时是需要的一些静态变量，类似于C语言中static关键字修饰的变量，对于无状态的operator来说，这部分可以为空。init定义了operator开始执行时需要进行的初始化工作。window规定输入、输出数据流缓冲区的窗口类型并决定operator对输入、输出流中数据访问的窗口大小。下面给出一个示例, 该示例描述了一个产生1,2,3,4,5,6...的自然数序列的数据流。

\begin{algorithm}\caption{产生自然数序列的数据流示例}\label{algo:operator-example}
  stream<int x>S;\\
  S = Source()\{\\
  \hspace*{1 pc} int i;\\
  \hspace*{1 pc} init \{i = 1;\}\\
  \hspace*{1 pc} work \{\\
  \hspace*{2 pc} S[0].x = i;\\
  \hspace*{2 pc} i++;\\
  \hspace*{1 pc} \}\\
  \hspace*{1 pc} window\{\\
  \hspace*{2 pc} S tumbling(1);\\
  \hspace*{1 pc} \}\\
  \};
\end{algorithm}

该示例的第1行描述了一个由整数int组成的数据流 S, 可以使用S[0]来访问数据流 S 的窗口中第一个位置, 而 S[0].x 表示该位置的成员 x (其类型为 int)。 接着，在示例的3~4行，Operator Source 声明了一个静态变量 i 并赋初值为1，接下来的执行流程为：执行一次 work 内部的语句->翻转窗口window->执行一次work内部语句->翻转窗口 window->…->执行一次work内部语句->翻转窗口window, 可以看出, 每次执行 work 都会使得窗口第一个位置的 x 赋值为自然数序列变量 i 的最新值 同时 i 自增,然后通过 window 中的窗口翻转将该数据传给后续的 Operator。

下面详细讨论COStream中的窗口机制。
\begin{figure}[htbp]
	\centering
	% Requires \usepackage{graphicx}
	\includegraphics[width=5in]{Img/Chap_Application/Yu/operator.png}\\
	\caption{operator的窗口机制}\label{fig:operator}
\end{figure}
COStream对流中的数据访问采用窗口机制，窗口用前沿和后沿界定，前沿和后沿的距离就是当前operator执行时可操作的输入、输出缓冲区的大小，它由window中的参数确定。COStream中窗口类型有两种：sliding和tumbling。sliding代表滑动窗口类型，这种窗口有peek，pop和push操作，一般用于输入流；而tumbling则代表翻转窗口类型，只有push和pop操作，该种类型窗口既可以用于输出流也可以用于输入流。对于输入流主要的操作有pop和peek。pop操作删除输入流中最先到的token，并返回该token。peek(i)返回输入流中距离输入流窗口前沿的第i个token。对于输出流主要操作是push，push操作是将计算得到的token输出到输出流，对缓冲区的使用是从前沿到后沿的。operator一次执行完成后同时移动窗口的前沿和后沿。图\ref{fig:operator}（a）图描述COStream中的输入输出流的窗口示意图，图\ref{fig:operator}（b）和（c）分别表示如果（a）对应的窗口分别是sliding(5,3)和tumbling(5)时，一次operator执行完成后窗口的移动情况。


\subsubsection{Composite}
在面向过程编程的 C 语言中, 程序入口为 main 函数, 在函数中使用多种控制语句来编写程序。与之不同，在面向数据流编程的 COStream 语言，程序入口为 Composite Main，在其内部通过将不同 Operator 用Stream 变量进行连接来构造数据流图。

COStream定义的用于连接不同节点构造数据流图的Composite结构属于高层次（相对于operator）的复合结构，代表一个由一个或若干个operator组成的可重用的数据流图结构，它是对SDF图中可复用的子图的抽象。它既可以完整的表示一个数据流程序的数据流图结构，也可以作为子数据流图结构被调用。下面是一个略去细节信息的抽象 composite 例子，它表示输入的数据流 S 依次经过了3个 operator 的处理后，计算得到了输出流T。

\begin{algorithm}\label{algo:operator}
Composite Main(input S, output T)\{\\
 \hspace*{1 pc} S0 = first(S);\\
 \hspace*{1 pc} S1 = second(S0);\\
 \hspace*{1 pc} T  = third(S1);\\
\}
\end{algorithm}

composite主要由composite头部和composite体组成。composite头部表明该composite的输入输出边参数、composite参数以及composite的名称，COStream不支持匿名composite。composite的输入输出边参数是用来确定在生成SDF图时该composite结构所形成的子图与SDF图中其他节点的连接关系。composite参数主要是指composite结构在实际被调用时需要从调用处传入的参数，可以根据参数的情况决定composite最终子图的结构，另外该参数也可以在composite内部被使用。composite体主要有如下2部分组成：一部分是composite内部需要使用的变量的定义和一些相关的操作语句；另一部分是composite内部的子图结构，它由一个或若干operator组成，每个operator根据输入流、输出流的依赖关系连接成图，是composite的核心结构。Composite结构在编译阶段被扩展，经过编译数据流程序的数据流图子结构将被扩展成完整的数据流图。

\subsubsection{多输入多输出流的连接方式}

composite可以同时有多个输入多个输出边。下面的数据流程序例子反映了上述语言特点。

\begin{algorithm}\label{algo:operator}
composite M (output K,L,input G,H) \{		//1\\
 \hspace*{1 pc} stream<int x> I=O(G)\{\}			//2\\
 \hspace*{1 pc} stream<int x> J=P(H)\{\}			//3\\
 \hspace*{1 pc} stream<int x> K=Q(I;J)\{\}			//4\\
 \hspace*{1 pc} stream<int x>L=R(J)\{\}			//5\\
\}									//6\\
composite Main\{						//7\\
 \hspace*{1 pc} …\\
 \hspace*{1 pc} (stream<int x>C,stream<int x> D) = M(A,B)\{\}	//8\\
 \hspace*{1 pc} (stream<int x>E,stream<int x> F) = M(A,B)\{\}	//9\\
 \hspace*{1 pc} …\\
\}
\end{algorithm}

\begin{figure}[htbp]
	\centering
	% Requires \usepackage{graphicx}
	\includegraphics[width=1.0\textwidth]{Img/Chap_Application/Yu/multi.png}\\
	\caption{多输入多输出流连接方式}\label{fig:multi}
\end{figure}

图\ref{fig:multi}描述了行1至行9的程序片段所反映的数据流图调用、扩展过程：由Original graph扩展为Expanded graph。Original graph包含了2个composite结构M，第一个M将输入流A和B计算后得到输出流C和D，第二个M将输入流A和B计算后得到输出流E和F。行1定义了M的输入流为G和H，输出流为K和L。行8表示当M在Main中第一次被调用时，输入流和输出流发生实例化：G=A,H=B,K=G以及L=D。在Expand graph中M因为被调用2次而实例化成2份，所以M中的中间流I和J也被实例化为2份：C.I，C.J和E.I，E.J。这些子流图因被调用而实例化展开的过程在编译器编译时完成，经过编译后，数据流程序的流图结构将被扩展成完整的数据流图。

\subsubsection{pipeline 连接方式}

\begin{figure}[htbp]
	\centering
	% Requires \usepackage{graphicx}
	\includegraphics[width=0.25\textwidth]{Img/Chap_Application/Yu/pipeline.png}\\
	\caption{pipeline连接方式}\label{fig:pipeline}
\end{figure}

有时候我们会想要编写这样一种数据流结构: 每个 operator 的输入恰好是上一个 operator 的输出,即数据流在全局或局部保持单向传递,有时 operator 还可以复用。例如对应图\ref{fig:pipeline}中有3个 operator 的示例：

\begin{algorithm}\label{algo:costream}
Composite Main(input<int x> S0)\{\\
 \hspace*{1 pc} stream<int x>S1,S2,S3;\\
 \hspace*{1 pc} S1 = operator0(S0);\\
 \hspace*{1 pc} S2 = operator1(S1);\\
 \hspace*{1 pc} S3 = operator2(S2);\\
\}
\end{algorithm}

在这种情况下, 我们最关心的是输入流变量 S0和输出流变量 S3,而数据流变量S1,S2是多余的信息，实际上我们并不关心中间变量。在其它函数式编程语言中，我们可以使用 compose(组合)这个操作符来描述这种结构：

\begin{algorithm}\label{algo:costream}
operator3 = compose(operator0,operator1,operator2)\\
S3 = operator3(S0)
\end{algorithm}

等价于

\begin{algorithm}\label{algo:costream}
S3 = operator2(operator1(operator0(S0)))
\end{algorithm}

那么在COStream 数据流编程语言中, 有没有这种”串联”形式的组合结构呢?答案是有的, 这就是 pipeline 结构, 下面给出对应示例:

\begin{algorithm}\label{algo:costream}
Composite Main(input<int x> S0)\{\\
 \hspace*{1 pc} S3 = pipeline(S0)\{\\
 \hspace*{2 pc} add operator0();\\
 \hspace*{2 pc} add operator1();\\
 \hspace*{2 pc} add operator2();\\
 \hspace*{1 pc} \}\\
\}
\end{algorithm}

pipeline结构是由一串连续的composite调用组成，它们在pipeline结构中出现的先后顺序决定了它们在数据流图中的依赖关系。pipeline是单输入流单输出流，COStream规定能够在pipeline中被调用的composite也必须是单输入流单输出流。

\begin{algorithm}
  \caption{代码片段 A 和使用了 for/if 的代码片段 B 等价}
  \label{algo: pipeline}
  {\bf 代码片段 A}\\
  Composite Main(input<int x> S0)\{\\
    \hspace*{1 pc} S3 = pipeline(S0)\{\\
    \hspace*{2 pc} add operator0();\\
    \hspace*{2 pc} add operator1();\\
    \hspace*{2 pc} add operator0();\\
    \hspace*{2 pc} add operator1();\\
    \hspace*{2 pc} add operator0();\\
    \hspace*{2 pc} add operator1();\\
    \hspace*{1 pc} \}\\
  \}\\
  {\bf 代码片段 B}\\
  Composite Main(input<int x> S0)\{\\
    \hspace*{1 pc} S3 = pipeline(S0)\{\\
    \hspace*{2 pc} for(int i=0;i<3;i++)\{\\
    \hspace*{3 pc} if(i\%2==0)\{\\
    \hspace*{4 pc} add operator0();\\
    \hspace*{3 pc} \}\\
    \hspace*{3 pc} else\{\\
    \hspace*{4 pc} add operator1();\\
    \hspace*{3 pc} \}\\
    \hspace*{2 pc} \}\\
    \hspace*{1 pc} \}\\
  \}
\end{algorithm}

COStream引入add操作将operator和composite调用添加到pipeline中。 此外， pipeline 的结构体中支持使用 if 、for、while 语句等来控制 operator 的组合， 如算法\ref{algo: pipeline}所示，我们可以把代码片段 A 改写为代码片段 B，二者是等价的。





\subsubsection{splitjoin 连接方式}

\begin{figure}[htbp]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.5\textwidth]{Img/Chap_Application/Yu/splitjoin.png}\\
  \caption{splitjoin连接方式}\label{fig:splitjoin}
\end{figure}

除了 pipeline结构, 还有一种 splitjoin 结构用来描述数据流的分解与合并（图\ref{fig:splitjoin}）。splitjoin结构由splitter、joiner以及一些composite调用组成，当一个流作为splitjoin结构的输入流时它被splitter分割，splitter根据splitjoin中composite调用情况产生一批满足相应条件的输出流作为不同的composite调用的输入流，这些composite调用的输出流作为joiner的输入流，joiner同样根据composite调用情况来合并各个输入流，将合并后产生的新的流作为joiner结点的输出，也是splitjoin结构的输出流。在splitjoin中splitter主要有两种方式：

\begin{itemize}	
  \begin{figure}[htbp]
    \centering
    % Requires \usepackage{graphicx}
    \includegraphics[width=0.8\textwidth]{Img/Chap_Application/Yu/duplicate.png}\\
    \caption{splitjoin-duplicate示意图}\label{fig:duplicate}
  \end{figure}

  \item {\bf duplicate方式}：该方式下所有的composite调用将会有完全相同的输入流（图\ref{fig:duplicate}）。

  \begin{figure}[htbp]
    \centering
    % Requires \usepackage{graphicx}
    \includegraphics[width=0.8\textwidth]{Img/Chap_Application/Yu/roundrobin.png}\\
    \caption{splitjoin-roundrobin示意图}\label{fig:roundrobin}
  \end{figure}

  \item {\bf roundrobin(w1,…,wn)方式}：如图\ref{fig:roundrobin}所示,该方式下splitter将其输入流中的前w1个数据发送给第一个composite调用，接下来的w2个数据发送给第二个composite调用，以此类推，将最后的wn个数据发送给最后一个composite调用。对于joiner只有一种roundrobin方式。与pipeline结构类似，在splitjoin中调用的composite要求也是单输入流和单输出流的。
\end{itemize}

在COStream中splitter和joiner分别用关键字split和join表示，且可以和 pipeline 结构互相嵌套使用，下面是一个示例：

\begin{figure}[htbp]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=1.0\textwidth]{Img/Chap_Application/Yu/vocodebank.png}\\
  \caption{vocodebank程序例子}\label{fig:vocodebank}
\end{figure}

图\ref{fig:vocodebank}的例子能够说明splitjoin和pipeline的层次性特点和使用特性。在图中左边部分是COStream的源码片段，右边是对应的数据流图。在本例中可以看到添加了层次性的结构有助于增加数据流程序编程的灵活性和可扩展性。另外，在splitjoin和pipeline中流能够被参数化，在本例composite VocodeBank中关于内置的pipeline调用次数可以有参数N来确定，通过N控制splitjoin结构的宽度，同样在pipeline中可以通过参数来控制pipeline的深度。

\subsubsection{内置 composite}

为了便于对文件操作，COStream对文件的I/O提供了内置composite支持，通过调用内置的文件I/O composite COStream可以完成对文件读写操作。FileReader和FileWriter是COStream为I/O提供的内置的composite接口。FileReader用于将文件中的数据读到流中，FileWriter是将流中的数据写到文件中。具体用法如下：

\begin{algorithm}\label{algo:costream}
stream<double x> A,B;			//1\\
A = FileReader(“data.bin”);		//2\\
FileWriter(B)(“result.txt”);
\end{algorithm}

行1表示将data.bin文件中的数据读入到数据流A中，行3表示将处理后输出的流B中的数据保存至文件result.txt。

此外，仍然存在许多常用功能性 Composite有待于内置在COStream编程语言中， 实验室的全体师生会积极进行版本迭代，进一步改进和优化COStream的编程语言结构设计。 

\subsection{COStream 编译器框架}

\begin{figure}[htbp]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.8\textwidth]{Img/Chap_Application/Yu/compiler.png}\\
  \caption{COStream 编译器框架示意图}\label{fig:compiler}
\end{figure}

图\ref{fig:compiler}描述了COStream编译系统总体框架。COStream编译器的源语言为COStream数据流程序设计语言，目标语言根据目标结构不同生成不同底层语言，如C、C++（在x86平台上执行）和openCL（在 GPU 上执行）以及 Javascript（在跨平台的浏览器中执行）等，编译器通过调用底层语言编译器生成对应目标平台的可执行文件。下面详细介绍COStream编译系统的组成部分。

\begin{itemize}	
  \item {\bf 编译器前端}：编译器前端主要对COStream源程序进行分析，建立抽象语法树。对源程序的分析主要包括词法分析和语法性分析。在词法分析中，词法分析器从左到右地读构成源程序的字符流，把字符流分组为多个记号，记号是具有整体含义的字符序列。词法分析通常作为编译器的第一个阶段，产生记号序列，提交给语法分析使用。这里将词法分析作为语法分析的子程序来实现。当词法分析器收到语法分析器发出“取下一个记号”的命令时，词法分析器读入字符，识别出一个记号。COStream采用自底向上的语法分析过程。在语法分析中，将字符串或者记号划分为具有一定层次的多个嵌套组，每个嵌套组具有一个完整的含义。程序的层次性结构是通过递归规则来表达，在编译器中，语法分析器接受词法分析器提供的记号串，检查它们是否符合COStream的文法规则。如果文法匹配则通过规则建立语法树结点结构。源程序通过词语法分析最终将建立为一种由顶层语法树节点表示的层次性抽象语法树结构。COStream采用Lex和Yacc词语法生成器生成词语法分析器。

  \item {\bf 语义分析和代码优化}: 编译器对经过词语法分析形成的抽象语法树进行语义分析。语义分析是对COStream数据流程序进行类型检查以及为代码生成阶段收集类型信息。语义检查负责检验每个操作符的操作数是否满足源语言的说明。在该阶段除语义检查外还对源程序代码做机器无关优化。优化是在保持源程序语义的基础上减少代码占用空间，删除不必要的操作，降低程序运行时开销。为了提升程序的性能COStream实现了常见的机器无关优化，如常量传播和冗余代码删除等。该阶段得到最佳抽象语法树。
  \item {\bf 中间代码生成}: COStream编译器以同步数据流图作为中间表示，在该阶段分析编译器前端得到的抽象语法树，得到数据流程序中的composite调用关系和stream依赖关系，利用这两种关系将抽象语法树进行深层次的展开，得到一个只有operator通过stream相连的完整的数据流图的抽象语法树结构，利用此时的抽象语法树生成SDF图，该SDF图是编译器后续操作的基础。另外，在该阶段还需要对SDF图中的各个节点的工作量进行估计，确定SDF图中各个actor的负载情况。
  \item {\bf 数据流图的周期性调度}: 编译器根据中间表示的数据流图结构采用单出现调度策略 (Single Appearance Schedule，SAS) 得到静态平衡数据流图。编译器根据程序中各个operator的peek、pop和push值决定初态和稳态调度阶段的各个actor的执行次数，为后续的优化和代码生成提供依据。
  \item {\bf 底层结构相关优化}: 编译器根据不同底层系统结构特性，对COStream程序进行与底层结构相关的优化，主要从计算任务分配、存储优化和通信优化等方面对程序进行优化，在开发并行性的同时减小相应的开销，使数据流程序在执行时能够充分利用底层系统的资源，提高程序的执行效率。COStream主要针对X86、GPU和多核集群等目标系统做编译优化。在第\ref{section:x86}节中具体介绍面向X86共享多核架构下的相关优化。
  \item {\bf 代码生成}: 根据底层结构相关优化的结果和后端的体系结构以及目标代码的特性，设计最终目标代码的生成框架，生成高效的并行代码。
  \item {\bf 运行时系统}: 运行时系统是采用目标语言编写的静态链接库，包括支持目标代码运行所需要通信库和函数库，为代码生成提供辅助支持。
  \item {\bf 底层编译器}: 编译器将生成的目标底层代码交由底层语言编译器编译生成指定目标平台的可执行文件，完成整个编译过程。
  
\end{itemize}

\subsection{面向 X86 多核集群的 COStream 编译优化} \label{section:x86}

随着多核处理器平台日趋复杂，数据流程序的并行编译优化工作变得更加困难，其问题主要包括多处理器核的并行调度和对主存的访问延迟。针对数据流程序共享存储多核架构的结构特点，构造出高吞吐量低延迟的软件流水线调度，主要分为任务划分、阶段赋值、流水线执行三个步骤\citep{唐九飞面向}。

\subsubsection{任务划分}

任务划分是在避免浪费处理器核的计算能力以及确保各处理器核间的任务负载均衡的基础上，使用合适的划分算法将数据流任务划分到合适的核上。任务划分的目的是根据数据流图（SDF图）中计算节点的负载和计算节点间的依赖关系构造高吞吐量低延迟的软件流水线调度，以最大化底层系统资源利用率。数据流程序要想充分利用系统资源就必须充分合理地利用其自身的数据并行性、任务并 行性和流水线并行性。在软件流水线调度中，流水线的启动间隔（Initiation Interval，II）是指相邻两次循环迭代进入流水线的时间间隔,II越小意味着吞吐率越大。任务划分的目的就是要最小化II。在软件流水中II是由流水线中各资源的处理时间决定，在X86中，资源主要是处理器核，那么影响程序最终执行性能的主要因素就是划分的负载均衡情况。

在进行任务划分前，需要对SDF进行预处理，包括扩大调度和融合相邻计算节点。扩大调度指的是以相同的扩大因子，成倍扩大每个计算单元稳态调度阶段的执行次数，这样可以减少同步开销对程序性能的影响。相邻计算节点的融合是指将两个独立且在数据流图中相邻的计算节点融合为一个计算单元。相邻计算单元融合操作保证了两个计算单元不会被分配到不同的处理器核上调度，这能够减少通信开销对程序的影响; 但是过度的融合操作会减少数据流图的计算单元，可能导致任务划分阶段负载的不均衡。相邻计算单元融合算法的关键在于如何判断是否对某对相邻计算单元进行融合。首先，被融合的计算单元必须是单输入单输出的，这是由于过多的通信边将导致复制分裂后计算单元间过多的通信开销;其次，因为存在状态计算单元不能进行数据并行调度，所以不对相应状态计算单元进行融合操作。对于满足上述两个条件的相邻节点，根据两者通信量之和以及融合后的计算量，当通信计算比大于某个阈值时，将其合并为一个计算单元。

在对SDF图进行预处理优化后，在此基础上采用划分算法来进行任务划分。COStream\citep{张维维2013COStream}任务划分采取以负载均衡为目标同时最小化同步通信开销的分配策略\cite{Wei2010Minimizing}。负载均衡的目的是为了保证流水线在满状态时各个核上的有效计算时间是相等的，核间因相互等待而产生的空闲时间将会小，II就会小，处理核利用率就高，系统的吞吐量就大。同时，因为SDF中计算节点之间有数据依赖，为了保证数据访问的局部性最小化同步通信延迟，在任务划分时应该尽可能地将有依赖关系的计算节点分配在一个核上以最大程度地减少核间通信边的数量。文献比较了常见的任务划分和分配策略，常见的分配策略主要包括有循环分发分配策略、亲和性分配策略和贪心分配策略等。综合COStream程序运行时存在的问题及常见任务分配策略，COStream选择以负载均衡为目标同时最小化通信开销的 K路图划分算法（MultilevelK-way Partitioning，MKP）。COStream采用Metis提供的接口实现MKP算法为SDF图作任务分配。由于MKP作为通用的图划分算法，没有充分结合数据流程序自身存在的各种并行性其结果并不能很好地满足程序运行的要求，COStream在MKP划分结果的基础上根据数据流程序的特点进行了进一步优化，提出复制分裂算法，该算法根据 MKP 划分结果的负载平衡情况，利用了SDF图中无状态的计算节点存在的数据并行性，对无状态的计算节点做分裂，增大任务并行性，将分裂产生的不同副本分配到不同的划分子集中，降低计算节点粒度，使不同划分的负载达到平衡。经过实验分析看到，COStream在X86环境下平衡因子设为1.5时能够得到较理想的结果，其中平衡因子指的是划分中负载最大的与最小的比值。

\subsubsection{阶段赋值}

\begin{algorithm}
  \caption{阶段赋值算法}
  \label{algo:stage}
  输入：SDF图G(V,E)，图G计算单元到核的映射actorProcMap\\
  输出：图G计算单元到阶段号的映射actorStageMap\\
  1.	topologicalOrder = topologicalTrav();\\
  2.	for all actor v in topologicalOrder do\\
  3.	  \hspace*{1 pc} int maxStage = 0; int stage;\\
  4.	  \hspace*{1 pc} for all actor u which is a parent of v do\\
  5.	    \hspace*{2 pc} if (actorProcMap[u] != actorProcMap[v]) then\\
  6.	    \hspace*{3 pc}   stage = actorStageMap[u] + 1;\\
  7.	    \hspace*{2 pc} else\\
  8.	    \hspace*{3 pc}   stage = actorStageMap[u];\\
  9.	    \hspace*{2 pc} endif\\
  10.	    \hspace*{2 pc} if ( stagemaxStage) then\\
  11.	    \hspace*{3 pc}   maxStage = stage ;\\
  12.	    \hspace*{2 pc} endif\\
  13.	  \hspace*{1 pc} end for\\
  14.	  \hspace*{1 pc} actorStageMap[v] = maxStage;\\
  15.	 end for
\end{algorithm}

阶段赋值是为了确定数据流计算节点分配到流水线的哪个阶段执行，在考虑各个计算节点之间的依赖关系的基础上，在空间维度上和时间维度上指定各个计算节点的相对计算顺序以满足计算节点间数据依赖关系。算法\ref{algo:stage}用伪代码描述了计算单元的阶段赋值算法，其以目标程序对应的数据流图和任务划分结果作为输入。对于数据流程序，其数据流图的有向边表示计算单元间的数据传输。算法首先构造数据流图中计算单元节点的拓扑排序以满足计算单元数据读写规则；然后以拓扑序列依次访问计算单元，根据数据读写节点是否被划分到同一个处理器核上确定计算单元的流水调度阶段值。算法\ref{algo:stage}描述了阶段赋值的过程。首先，对数据流图G(V, E)进行拓扑排序；其次，按照数据依赖关系依次访问所有的actor，对u∈ V，令maxStage为0，遍历u的每个输入边（v，u）∈E，如果v和u在不同处理核上，则u的阶段号stageu = stagev + 1；否则u的阶段号stageu = stagev。如果stageu > maxStage，则更新maxStage。在遍历完u的每条边后，以maxStage作为u的阶段号

\subsubsection{流水线执行}

\begin{figure}[htbp]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=1.0\textwidth]{Img/Chap_Application/Yu/streamline.png}\\
  \caption{(a)任务划分和阶段赋值的例子 (b)图(a)对应的软件流水执行过程}\label{fig:streamline}
\end{figure}

流水线执行过程\citep{WeiSoftware}分为三个阶段，分别是填充阶段、满阶段以及排空阶段。图\ref{fig:streamline}给出了任务划分、阶段赋值和软件流水调度执行的一个例子。初始SDF图 有Ａ、Ｂ和Ｃ这３个计算节点，在任务划分阶段将Ｂ复制分裂成两个计算单元Ｂ１和Ｂ２，同时引入了split和join两个计算节点。经过任务划分后，计算单元Ａ被分配到处理器核core0，split和B1被分配到core1，B2、join和Ｃ被分配到core2上。图\ref{fig:streamline}（ａ）给出了经过阶段赋值后各个计算单元的阶段值，完成了软件流水的构造。图\ref{fig:streamline}（ｂ）给出了对应的软件流水调度执行过程，程序启动时流水线处于填充阶段，各个核上的计算节点按照其阶段值从小到大的顺序启动执行，当所有的计算节点都启动时，流水线进入满阶段；在流水线满阶段，所有的计算节点都进行周期性的迭代执行，由于任务划分阶段基本实现了不同计算核上的负载均衡，此时核间同步开销小，资源利用率高，系统的吞吐率达到最大；在流水线调度的排空阶段，程序开始相继结束计算节点的执行，各个核的计算节点按照阶段值从小到大的顺序结束其周期性的迭代，等到所有的计算节点结束其稳态调度后，整个程序终止并释放运行时占有的各种资源。

\subsubsection{性能评估}
为了对COStream编译系统面向X86架构的编译优化进行全面的性能评估和分析，选取了11个多媒体领域常见的算法作为测试程序用COStream数据流编程语言实现作为COStream编译器的输入。
\begin{figure}[htbp]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=1.0\textwidth]{Img/Chap_Application/Yu/x86benchmark.png}\\
  \caption{用于测试的数据流程序信息}\label{fig:x86benchmark}
\end{figure}

图\ref{fig:x86ratio}给出了测试程序经过COStream编译后在目标环境上执行的加速比。从图中可以看出，每个程序执行加速比随着处理核的数目增多而增加，基本呈现一个线性的加速情况。
\begin{figure}[htbp]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.8\textwidth]{Img/Chap_Application/Yu/x86ratio.png}\\
  \caption{测试代码在X86上达到的加速比}\label{fig:x86ratio}
\end{figure}

通信与同步比衡量了测试程序的有用计算和时间开销之间的比例，在一定程度上反应了程序的性能。在X86环境下采用的是软件流水调度策略，各个并行线程在软件流水调度的每次执行阶段中需要进行一次同步操作，保证不同核上的actor间的数据依赖关系能够满足。图\ref{fig:x86computeSync}给出了各个测试程序在8个核上的运行时计算同步时间分布。从图\ref{fig:x86computeSync}可以看出，FMRadio和Vocoder的计算同步比不太理想，同步开销太大，导致加速比较低，对于其他的测试程序计算同步比都能够达到较高水平，程序的加速比也较为理想。

\begin{figure}[htbp]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.8\textwidth]{Img/Chap_Application/Yu/x86computeSync.png}\\
  \caption{测试程序在8核上运行的计算同步比}\label{fig:x86computeSync}
\end{figure}

\subsection{面向 CPU 和 GPU 异构多核集群的 COStream 编译优化}

目前，各种新型的高性能体系结构不断涌现，以传统处理器CPU与图形处 理器GPU协同工作模式来构建异构硬件平台逐渐成为一种新型的并行处理方 式\citep{weihaitao2010}。CPU/GPU异构集群是由多个服务器节点组成的集群，其中每个节点封装多 核CPU与一定数量的GPU。CPU/GPU异构集群相对通用CPU集群在计算、访存与通信方面存在很大的差异性，这给研究人员带来了难度。 

COStream在异构集群的数据流程序的编译优化上，提出并实现了面向 CPU/GPU混合架构的数据流程序任务划分方法和多粒度调度策略，包括任务的分类处理、GPU端任务的水平分裂和CPU端离散任务的均衡化，构造了软件流水调度，经过编译优化生成OpenCL目标代码。

GPU在大规模数据的并行方面有很大的优势，但GPU间的通信开销却相当巨大，任务划分目的主要是保证各服务器节点的计算负载均衡、通信开销最小化，以及在单个节点上，充分发挥CPU逻辑控制与串行计算特点以及GPU计算密集性优势，同时保证 CPU 多核间负载均衡和不同GPU间数据通信开销最小化。COStream在异构集群的数据流程序的编译框架中的任务划分算法主要分为任务划分预处理、任务划分与调度\citep{陈文斌基于}。

\subsubsection{任务划分预处理}
该阶段主要分成两个步骤：一是引入扩大因子，二是任务的分类处理。引入扩大因子，即计算节点执行次数整体扩大，在各个应用程序中，各计算节点的执行次数远远小于GPU可并行执行的规模，采用执行次数的整体扩大的方法，增大各个计算节点的工作量，从而充分利用GPU的并行计算单元；扩大因子的上限由三个因素决定：主存大小（Host Memory）、GPU存储空间（Device Memory）和GPU缓冲区分配机制。

CPU端允许扩大倍数为N1，则
\begin{equation}
  N_1 \leq M_{CPU} / M_{CPUSumneeded} \label{eq:2.1}
\end{equation}

其中，MCPU表示主存可用空间的大小，MCPUSumneeded表示CPU端所有任务未扩大前所需的存储空间总和。

GPU端允许扩大的倍数为N2，N2需满足公式\ref{eq:2.2}和公式\ref{eq:2.3}。
\begin{equation}
  N_2 \leq M_{GPU} / M_{GPUSumneeded} \label{eq:2.2}
\end{equation}

其中，$M_{GPU}$表示GPU可用的存储空间，$M_{GPUSumneeded}$表示GPU端所有任务未扩大前所需的存储空间总和。

\begin{equation}
  N_2 \leq M_{GPU} / M_{GPUSumneeded} \label{eq:2.3}
\end{equation}

其中，$M_{GPUmaxbuffer}$表示GPU每次允许分配缓冲区的最大值，$M_{GPUmaxneeded}$表示GPU端所有任务未扩大前各需存储空间的最大值。

在同时满足公式\ref{eq:2.1}、公式\ref{eq:2.2}、公式\ref{eq:2.3}的前提下，选取合适的扩大因子为来保证GPU端任务有足够的计算量，并充分利用GPU计算资源。同时，引入扩大因子的过程间接地减少了CPU线程的同步次数，从而降低了应用程序整体的同步开销。

任务的分类处理，该步骤根据任务是否具有可并行性以及相应通信开销的大小将其分配到GPU或CPU上去运行，充分利用CPU与GPU各自的优势，同时降低整体程序的通信量。COStream数据流程序中，计算节点有两种状态：有状态（Stateful）和无状态（Stateless）。状态为Stateful的结点表示该计算结点连续两次迭代运算间存在数据依赖，两次迭代运算不可并行执行；状态为Stateless的结点则与之相反，多次换代运算之间互相独立，可并行执行。数据流程序任务的分类处理，目标是根据计算节点的计算特点，将计算节点分配到适合的计算平台上进行计算，在分类的过程中，既要考虑该计算任务是否具有可并行性，即是否为状态是Stateless的结点，同时也要考虑GPU间高额的通信开销，减小异构平台间的数据通信量。任务分类首先判断SDF图中各个计算节点的状态，将Stateless计算节点划分到GPU端，Stateful计算节点划分到CPU端。接着考虑异构平台间的通信开销，调节边界结点。对于划分到GPU端的Stateless结点，如果其父结点为Stateful结点，并且其子结点为Stateful结点，那么初步分类的结果就会带来大量的GPU间的数据通信，从而产生巨大的通信开销，影响数据流程序的整体性能，因此任务初步分类完成后，需要将此类计算节点微调到CPU端。

\subsubsection{任务划分}
基于GPU/CPU混合架构的数据流程序任务划分包括两部分内容——GPU端任务的水平分裂（GPU Tasks Horizon Splitting, GTHS）和CPU端离散任务均衡化（CPU Disperse Tasks Balancing, CDTB）。

对于任务划分，它既要满足负载均衡，又要降低通信开销，常用的针对SDF图的划分方法有METIS划分工具、K路图划分方法等。METIS划分与K路划分方法都满足负载均衡和减小通信开销的目标，由于GPU与GPU间数据通信开销的巨大，导致这两种划分方法对应用程序的整体执行效率提高不显著。

针对上述问题，GTHS算法利用任务的并行性将其均衡分裂到各GPU，以防止GPU与GPU间高额的通信开销影响数据流程序的整体性能。算法\ref{algo:gths}描述了GPU端任务水平分裂的具体实现过程，该方法主要分为两个阶段完成：一是GPU端actor的分裂，完成任务的分类处理和扩大因子的引入后，划分到GPU端的任务各自的迭代计算具有很高的并行性，但针对一些特殊的应用程序，其GPU端任务间的数据访问对前一次的运行结果有依赖性，对于这种特殊类型的应用程序不进行GPU端任务的水平分裂，而将分配到GPU端的任务均由一个GPU计算完成；该划分方法针对GPU端任务间数据访问对前一次的运行结果无依赖性的应用，遍历分配到GPU端的各actor，修改各actor的稳态执行次数，并创建M个新actor（M为GPU的个数减1），将该actor复制给M个新创建的actor（称该actor为新创建的M个actor的模板结点，新创建的M个actor为该actor的复制结点），并修改对应新结点的名字(行1-12)；二是对应的新结点连接，actor分裂完成后，新创建的actor是离散的，将各离散的新创建的actor对应连接起来，形成新的连通的SDF图（行13-42）。依次遍历每一个新创建的actor，如果其模板actor的父结点是GPU端actor，则该新创建的actor的模板结点的父结点在上一阶段也进行了水平分裂，那么将该新创建的actor与其模板actor的父结点的相应复制结点连接起来；如果其模板actor的父结点是CPU端actor，则其模板actor的父结点在上一阶段未进行水平分裂，那么将该新创建的actor与其模板actor的父结点直接连接起来（行14-27）；如果其模板actor的子结点是GPU端actor，则该模板结点的子结点在上一阶段也进行了水平分裂，那么需要将该新创建的actor与其模板actor的子结点的相应复制结点连接起来；如果其模板actor的子结点是CPU端actor，则该模板actor的子结点在上一阶段未进行水平分裂，那么需要将该新创建的actor与其模板actor的子结点直接连接起来（行28-41）。如图\ref{fig:3.1}所示为数据流程序GPU端任务水平分裂的结果示意图，图\ref{fig:3.1}(a)为任务的分类结果，计算节点Ａ和计算节点Ｆ为CPU端结点计算节点Ｂ、Ｃ、Ｄ、Ｆ为GPU端结点；图\ref{fig:3.1}(b)为GPU端任务水平分裂结果，其中，计算节点Ａ和计算节点Ｆ为CPU端结点，计算节点B1、C1、D1和E1划分到GPU0，计算节点B2、C2、D2和E2划分到GPU1，计算节点B3、C3、D3和E3划分到GPU２。

\begin{breakablealgorithm} \label{algo:gths}
  \caption{GPU端任务水平分裂算法}
  {\bf Input: GPUNode[ ], SDF graph}\\
  {\bf Output: new SDF graph}\\
  1 for all actor v in GPUNode[] do\\
  2   \hspace*{1 pc} v.executions /= GPUNumber;\\
  3   \hspace*{1 pc} int NewActorNumber = GPUNumber – 1;\\
  4   \hspace*{1 pc} int index = 1;\\
  5   \hspace*{1 pc} multimap mapActor2NewActor;\\
  6   \hspace*{1 pc} for int i = 0; i < NewActorNumber; ++i do\\
  7   \hspace*{2 pc} Actor m = CreateNewActor(v);\\
  8   \hspace*{2 pc} m.name += itoa(index);\\
  9   \hspace*{2 pc} mapActor2NewActor.insert(v,m);\\
  10  \hspace*{2 pc} ++index;\\
  11  \hspace*{1 pc} end for\\
  12end for\\
  13for all actor u in GPUNode[] do\\
  14  \hspace*{1 pc} if(IsGPUNode(u.parent))then\\
  15  \hspace*{2 pc} int index = 1;\\
  16  \hspace*{2 pc} for ;index < GPUNumber;++index do\{\\
  17  \hspace*{3 pc} Actor a = FindNewActor(u,index);\\
  18  \hspace*{3 pc} Actor b= FindNewActor(u.parent,index);\\
  19  \hspace*{3 pc} a.parent = b;\\
  20  \hspace*{2 pc} end for\\
  21  \hspace*{1 pc} else\\
  22  \hspace*{2 pc} int index = 1;\\
  23  \hspace*{2 pc} for ; index < GPUNumber; ++index do\\
  24  \hspace*{3 pc} Actor a = FindNewActor(u,index);\\
  25  \hspace*{3 pc} a.parent = u.parent;\\
  26  \hspace*{2 pc} end for\\
  27  \hspace*{1 pc} end if\\
  28  \hspace*{1 pc} if(IsGPUNode(u.child))then\\
  29  \hspace*{2 pc} int index = 1;\\
  30  \hspace*{2 pc} for ;index < GPUNumber;++index do\\
  31  \hspace*{3 pc} Actor a = FindNewActor(u,index);\\
  32  \hspace*{3 pc} Actor b= FindNewActor(u.child,index);\\
  33  \hspace*{3 pc} a.child = b;\\
  34  \hspace*{2 pc} end for\\
  35  \hspace*{1 pc} else\\
  36  \hspace*{2 pc} int index = 1;\\
  37  \hspace*{2 pc} for ; index < GPUNumber; ++index do\\
  38  \hspace*{3 pc} Actor a = FindNewActor(u,index);\\
  39  \hspace*{3 pc} a.child = u.child;\\
  40  \hspace*{2 pc} end for\\
  41  \hspace*{1 pc} end if\\
  42end for
\end{breakablealgorithm}


\begin{figure}[htbp]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.8\textwidth]{Img/Chap_Application/Yu/3-1.png}\\
  \caption{GPU端任务水平分裂示意图}\label{fig:3.1}
\end{figure}


GPU/CPU混合架构平台下，GPU端任务的水平分裂完成了GPU端任务的划分，而原有的任务划分方法\citep{魏海涛一种面向数据流程序的软件流水并行化方法}对任务分类处理后分配到CPU端的各个离散结点未进行划分处理，均由一个CPU核控制计算，如果该CPU核的工作量过大时就会影响整个程序的执行效率。数据流程序CDTB算法针对这些CPU端的离散结点，结合复制分裂算法，选择合适CPU核，采用贪心的思想，均衡地将各计算结点分成若干个划分并将各划分映射到相应的CPU核上，保证各CPU核的负载均衡并提高各CPU核的利用率。基于GPU/CPU混合架构的数据流程序CPU端离散任务的均衡化过程主要包括以下三个阶段：

\begin{itemize}	
  \item {\bf CPU核数的确定}：图\ref{fig:3.2}为CPU核数确定过程的流程示意图。分别计算CPU端任务总的执行时间CPUTime、GPU端任务总的计算时间GPUTime以及数据流程序总的通信时间CommTime，如果CPUTime小于GPUTime和CommTime的最大值，选择CPU的核数为1，即分配到CPU端所有actor均由一个CPU核控制执行；如果CPUTime大于GPUTime和CommTime的最大值且分配到CPU端的actor个数为1，该actor的状态为stateful即该actor不可进行分裂，则选择CPU的核数为1；如果分配到CPU端的actor个数大于1或者存在工作量较大的状态为stateless的actor，根据公式\ref{eq:3.1}计算出应用程序执行时所需要的CPU核数NCPUcore；如果所求得的CPU核数大于系统可用的核数，则设置所需的CPU核数为系统可用的核数。通过CPU核数的确定，充分利用系统的计算资源，减小系统计算资源的浪费。

  \item {\bf CPU端工作量相对较大且状态为stateless的actor复制分裂}: 根据CPU端所有actor总的工作量CPUTotalWork和计算出所需的CPU核数NCPUcore，结合公式\ref{eq:3.2}求得理论上分配给每个核的最大工作量MaxTotalWork，遍历CPU端的状态为stateless的actor，根据工作量估计求出该actor的工作量大小，如果其工作量大于MaxTotalWork，则利用复制分裂算法对该actor进行复制分裂操作，将新生成的actor与其模板actor的父结点和子结点连接以保证SDF的连通性，经过多次反复的复制分裂操作使CPU端每个状态为stateless的actor的工作量都小于MaxTotalWork。
  
  \begin{equation}
    N_{CPUcore} = CPUTime/Max(GPUTime,CommTime) \label{eq:3.1}
  \end{equation}

  \begin{equation}
    MaxTotalWork = CPUTotalWork/N_{CPUcore} \label{eq:3.2}
  \end{equation}

  \item {\bf CPU端各离散的actor与CPU核间的映射}: 该过程采用贪心的思想，遍历CPU端actor并根据工作量估计计算出该actor的工作量，选择当前各自总工作量最小的CPU核，将该actor划分到当前总工作量最小的CPU核上，完成该actor与CPU核映射。算法\ref{algo:cpureflect}描述了CPU端离散任务均衡化具体过程的实现细节，该方法适合于CPU端计算成为软件流水调度过程中数据流程序整体效率瓶颈的情况。根据公式\ref{eq:3.1}和公式\ref{eq:3.2}分别计算出该应用程序所需的CPU核数NCPUcore和每个CPU核理论最大的总的工作量MaxTotalWork，遍历CPU端所有状态为stateless的actor结点，针对工作量workvalue大于MaxTotalWork的actor，通过CreateNewActor函数对该actor进行分裂操作并将新创建的actor与其模板actor的父结点和子结点连接（行1-11）。遍历所有CPU端actor，选择当前总的工作量估计最小的CPU核并完成该actor与该CPU核的映射（行12-17）。
\end{itemize}

\begin{figure}[htbp] 
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.8\textwidth]{Img/Chap_Application/Yu/3-2.png}\\
  \caption{CPU核数确定过程的流程示意图}\label{fig:3.2}
\end{figure}

\begin{algorithm}
  \caption{CPU端离散任务均衡算法}
  \label{algo:cpureflect}
  Input: CPUNode[]\\
  Output: mapActor2Partition(actor:partition)\\
  for( all actor v in CPUNode[] ) \{\\
    \hspace*{1 pc} int workvalue = CalculateWorkValue(v);\\
    \hspace*{1 pc} if(!DetectiveActorState(v) \&\& workvalue > MaxTotalWork)\{\\
    \hspace*{2 pc} int number = workvalue / MaxTotalWork;\\
    \hspace*{2 pc} for(int i = 0; i < number; ++i) \{\\
    \hspace*{3 pc} Actor u = CreateNewActor(v);\\
    \hspace*{3 pc} u.parent = v.parent;\\
    \hspace*{3 pc} u.child = v.child;\\
    \hspace*{2 pc} \}\\
    \hspace*{1 pc} \}\\
  \}\\
  for( all actor w in CPUNode[] )\{\\
    \hspace*{1 pc} int workvalue = CalculateWorkValue(w);\\
    \hspace*{1 pc} int index = FindMinWorkvalueCore();\\
    \hspace*{1 pc} UpdateWorkvalue(index);\\
    \hspace*{1 pc} mapActor2Partition(w:index);\\
  \}
  \end{algorithm}

\subsubsection{阶段赋值}
基于GPU/CPU混合架构的数据流程序优化框架采用软件流水调度方式，主要解决了软件流水调度过程中的三个问题：

\begin{itemize}	
  \item {\bf 线程同步问题}：在软件流水调度\citep{张维维2014面向多核集群的数据流程序层次流水线并行优化方法研究,杨胜哲数据流程序动态调度与优化方法研究,唐九飞面向}过程中，每个线程执行完各自的计算任务后，都需要进行一次同步操作，保证各个线程处于同一调度阶段，虽然该同步开销比硬件调度方式下的同步开销小，但由于计算任务的工作量过小或者处理器核数的增加都有可能带来同步开销的增大，从而影响了应用程序的执行效率。该优化框架引入扩大因子，成倍的扩大计算任务的稳态执行次数，增大各个计算任务的工作量，使计算任务多次计算后，同步一次，从而大大地减小了程序的同步操作数，增加了计算同步比，同时也充分利用了GPU/CPU混合架构下GPU高度并行计算的优势，提高了数据流程序的执行效率。

  \item {\bf 负载均衡问题}: 软件流水调度过程中，负载均衡是影响应用程序整体执行效率的重要因素。在GPU/CPU混合架构系统平台下，由于GPU与CPU计算能力的不对等，如果各个线程的负载不均衡，则会带来多个线程忙等的现象，增加程序的同步开销。该优化框架通过GPU端任务水平分裂算法和CPU端离散任务均衡化算法，实现了GPU端计算、CPU端计算和异构通信的相对均衡，同时降低了应用程序的通信量。
  
  \item {\bf 通信开销问题}: 在GPU/CPU混合架构中，异构通信占用软件流水调度的一个阶段，达到软件流水满状态，即计算任务稳态执行阶段，CPU端任务的计算、GPU端任务的计算以及CPU与GPU间的数据通信是并行执行的，高额的通信开销也会降低应用程序的执行效率。该优化框架采用任务分类方法，结合任务的计算特点，完成任务的分类，同时也降低异构的通信开销；GPU端任务的水平分裂，将GPU端任务均衡分裂到各GPU，避免了GPU与GPU间的高额通信，达到降低通信开销的目的。
\end{itemize}

经过GPU端GTHS算法和CPU端CDTB算法的任务划分，SDF图中的actor均被划分到对应的计算单元上运行，挖掘状态为stateful类型的结点的流水线并行性，构造软件流水线调度，使得状态为stateful的结点连续两次迭代计算，结点计算和数据传输都可以重叠执行，需要确定各结点被流水调度执行时的阶段号，即进行阶段赋值。

\begin{algorithm}
  \caption{基于GPU/CPU混合架构的阶段赋值}
  \label{algo:gpucpu}
  Input: SDFgraph, CPUNode[],GPUNode[], mapActor2Partition(actor:partition)\\
  Output: mapActor2Stage(actor:stage)\\
  TopologyOrderofActors = TopologyTravSDF();\\
  int MaxStage = 0;\\
  int tempStage;\\
  for all actor v in TopologyOrderofActors \{\\
    \hspace*{1 pc} for all actor u which is parent of v \{\\
    \hspace*{2 pc} if((IsCPUNode(v) \&\& IsCPUNode(u)) || IsGPUNode(v) \&\& IsGPUNode(u))\{\\
    \hspace*{3 pc} if(mapActor2Partition[v] == mapActor2Partition[u])\{\\
    \hspace*{4 pc} tempStage = mapActor2Stage[u];\\
    \hspace*{3 pc} \}else\{\\
    \hspace*{4 pc} tempStage = mapActor2Stage[u] + 1;\\
    \hspace*{3 pc} \}\\
    \hspace*{2 pc} \}else\{\\
    \hspace*{3 pc} tempStage = mapActor2Stage[u] + 2;\\
    \hspace*{2 pc} \}\\
    \hspace*{2 pc} if(tempStage > MaxStage)\{\\
    \hspace*{3 pc} MaxStage = tempStage;\\
    \hspace*{2 pc} \}\\
    \hspace*{1 pc} \}\\
    \hspace*{1 pc} mapActor2Stage[v] = MaxStage;\\
  \}
\end{algorithm}

算法\ref{algo:gpucpu}描述了面向GPU/CPU混合架构的数据流程序阶段赋值过程。对SDF图进行拓扑排序以满足数据的读写规则（行1），依次遍历拓扑排序中的各actor，如果该actor与其父结点同在CPU端或同在GPU端，并且属于同一个划分，那么该actor阶段号与其父结点阶段号相同；如果该actor与其父结点同在CPU端或同在GPU端，但是不属于同一个划分，那么该actor阶段号等于其父结点阶段号加1；如果该actor与其父结点不同在CPU端或GPU端，那么该actor阶段号等于其父结点阶段号加2（行4-21），因为该actor与其父结点不同在CPU端或不同在GPU端则会产生通信，数据通信在软件流水调度过程中需要占用一个阶段号。图\ref{fig:gpucpu}所示为基于GPU/CPU混合架构的阶段赋值结果，其中，actor A、B、C为CPU端结点，actor D、E、F、G、I为GPU端结点，在CPU端，actor A为一个划分，actor B和C属于同一个划分，actor A的阶段号为0，actor B与actor A都是CPU端结点，但不属于同一划分，根据算法\ref{algo:gpucpu}，actor B的阶段号为actor A的阶段号加上1；actor C和actor B都是CPU端结点，并且属于相同的划分，因此，actor C的阶段号等于actor B的阶段号；actor D与actor C分配在不同的平台，即不同在GPU或CPU，actor D的阶段号为actor C的阶段号加上2，因为actor C与actor D的数据通信需要占用一个阶段号，由于GPU的并行规模比较丰富，实验中将分配到GPU并且连通的结点划分同一个阶段执行。


\begin{figure}[htbp]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.8\textwidth]{Img/Chap_Application/Yu/gpucpu.png}\\
  \caption{基于GPU/CPU混合架构的阶段赋值}\label{fig:gpucpu}
\end{figure}

在三块NVIDIA Tesla C2050、两块四核CPU为混合架构系统结构上进行测试（图\ref{fig:fusion}）。选取9个数字多媒体领域的典型算法作为测试程序。

\begin{figure}[htbp]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.8\textwidth]{Img/Chap_Application/Yu/fusion.png}\\
  \caption{混合架构系统结构}\label{fig:fusion}
\end{figure}

图\ref{fig:gm-commu}和图\ref{fig:gths-metis}显示了比较GPU端任务水平分裂算法与通用METIS划分算法的通信量以及执行时间的结果。可见在GPU/CPU混合架构平台下GTHS算法的执行效果优于METIS划分方法。

\begin{figure}[htbp]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.8\textwidth]{Img/Chap_Application/Yu/gths-metis-commu.png}\\
  \caption{GTHS与METIS划分通信量比较}\label{fig:gm-commu}
\end{figure}

\begin{figure}[htbp]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.8\textwidth]{Img/Chap_Application/Yu/gths-metis.png}\\
  \caption{GTHS与METIS划分执行性能比较}\label{fig:gths-metis}
\end{figure}

对于CPU端任务较重的情况采用CDTB方法进行任务划分，比较CPU端离散任务均衡化算法与传统CPU端单核计算的执行时间。可见CDTB方法更适合于CPU端任务较重且状态为stateful的actor的计算量较小的应用程序。

在GPU/CPU混合架构平台上，采用CPU与GPU协作计算的模式，充分发挥各计算单元的优势，CPU负责控制主程序的流程和串行计算，准备GPU端计算所需的相应的数据并将其传输给GPU，充分利用GPU进行高效的并行计算。图\ref{fig:5.4}为数据流程序分别采用CPU单核和1个GPU协作计算的模式与CPU单核计算并执行1000次的性能比较示意图，测试程序Beamformer中存在大量离散的状态为stateful的结点，导致执行过程中产生大量的CPU与GPU的数据通信，高额的数据通信开销使程序的性能只提高6.8，相对加速效果不明显。而对于测试程序FFT和ChannelVocoder，程序内部的通信开销相对GPU的计算开销很小，充分利用了GPU强大的并行计算规模，所以加速效果显著。

\begin{figure}[htbp]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.8\textwidth]{Img/Chap_Application/Yu/5-4.png}\\
  \caption{CPU与GPU协作计算相对CPU单核的性能比较}\label{fig:5.4}
\end{figure}

图\ref{fig:3gpu-1gpu}所示为数据流程序分别采用三个GPU与一个GPU并执行1000次的执行性能的比较示意图。从图中可以看出，对于测试用例BeamFormer和BitonicSort，CPU端任务计算量较重，远远大于GPU端的运行时间，采用三个GPU进行并行计算，相对于采用一个GPU计算的加速效果不明显。而对于测试用例Serpent\_full和DES，CPU端任务计算量较轻，远小于GPU端任务的计算时间，采用三个GPU并行计算的加速效果比较显著。

\begin{figure}[htbp]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.8\textwidth]{Img/Chap_Application/Yu/3gpu-1gpu.png}\\
  \caption{三个GPU与一个GPU执行性能比}\label{fig:3gpu-1gpu}
\end{figure}

\subsection{基于 COStream 的深度学习程序开发实例}

深度学习模型本质是数学模型，在基于COStream实现深度学习模型时\citep{wangzhaoji}，只要将深度学习中的数学逻辑拆分成独立的计算节点并明确它们之间的数据依赖关系，其实不难实现。神经网络由神经元组成，在设计神经网络时，需要考虑网络的整体结构，应该有多少神经元，以及这些神经元应该如何相连。大多数神经网络被组织成称为层的单元组，如全连接层、卷积层等，不同类型的层连接方式不同，大多数神经网络将这些层布置成链式结构。后文将按照层来拆分整个网络模型\citep{adams}。

神经网络模型的训练主要有两个阶段，正向传播和反向传播。尽管深度学习中每一类层对应的计算节点的数学逻辑不同，但是其正向传播和反向传播对应的计算节点的输入流和输出流遵循相同的规则。

对于正向传播，其计算过程是多个函数的复合，假设模型中每一层对应的函数依次为$f^{\left (1 \right )}$、$f^{\left (2 \right )}$、$f^{\left (3 \right )}$、$f^{\left (4 \right )}$，则复合成一条$f\left ( x \right )= f^{\left (4 \right )}\left ( f^{\left (3 \right )}\left ( f^{\left (2 \right )}\left ( f^{\left (1 \right )}\left ( x \right ) \right ) \right ) \right )$的计算链，其中x代表输入层的输入，相邻层之间存在数据依赖，形成流水线。

对于反向传播，其计算过程也是多个函数的复合，利用链式求导法则，按照网络模型反向逐层计算并传递误差（由于正向传播和反向传播计算过程中层的计算顺序是相反的，因此在下文中将始终按照网络模型中定义的层间关系来表述），需要分别计算该层参数和该层输入关于损失函数的偏导数，该计算依赖于模型中该层下一层传入的输入误差、正向传播时该层的输入值和参数矩阵。最终，根据误差更新参数。

正向传播和反向传播各层间在时间维度上是串行关系，可以用数据流编程模型提供的软件流水线并行方式进行处理。

对于模型中的参数，由于计算除依赖于上一个计算节点输出的数据流，还依赖于网络模型中的随着计算迭代更新的参数，因此将各层节点的参数定义为全局变量。

因此，层的正向传播阶段计算节点有且仅有一条输入流，数据流内为上一层正向传播阶段计算节点的输出或由训练集数据；有两个输出流，将计算结果分别传递给下一层对应的正向传播和反向传播计算节点。层在反向传播阶段的计算节点有两个输入流，其一为下一层反向传播阶段计算节点计算的误差，其二为正向传播过程中传入该层的输入；有一个输出流，将该层计算的误差继续传递上一层反向传播阶段的计算节点。

对于一个包含三个层的神经网络，它的数据流图如图\ref{fig:dataflow_graph_of_net_with_3_layers}。
\begin{figure}[!t]
\centering
\includegraphics[width=0.8\textwidth]{../img/Chap_Application/Yu/dataflow_graph_of_net_with_3_layers.png}
\caption{包含三个层的神经网络的数据流图}
\label{fig:dataflow_graph_of_net_with_3_layers}
\end{figure}

接下来选用MNIST数据集，其训练集是由来自 250 个不同人手写的数字构成的单通道$28\ast28$图片，分别设计全连接神经网络和卷积神经网络解决识别手写数字的问题。

\subsubsection{全连接神经网络的并行化分析与设计}
全连接神经网络由全连接层组成，全连接层中的每一个神经元都与上一层中所有神经元相连，神经元将输出它所在层的上一层中所有与其相连的神经元同相连边上权值相乘后累加的结果，若对该层设置了激活函数，如ReLU函数、Sigmoid函数等，则该层中所有神经元将输出上述累乘求和结果关于该激活函数的映射，如图\ref{fig:neure}。因此，全连接层用来将该层之前的层提取到的特征综合起来。

\begin{figure}[!t]
\centering
\includegraphics[width=0.7\textwidth]{../img/Chap_Application/Yu/neure.png}
\caption{神经元}
\label{fig:neure}
\end{figure}

现在基于MNIST数据集解决手写数字识别问题，由于数据集中单张图片的像素数为784($28\ast28$)，所以输入层的神经元个数为784，设计该全连接神经网络共包含四层全连接层，其中前三层每层包含20个神经元，最后一层神经元的数量为10，依次对应手写图片为数字0到9预测概率。图\ref{fig:dnn}为该全连接神经网络模型。

\begin{figure}[!t]
\centering
\includegraphics[width=0.8\textwidth]{../img/Chap_Application/Yu/dnn.png}
\caption{四层全连接神经网络}
\label{fig:dnn}
\end{figure}

全连接层正向传播过程执行的计算实际为向量乘矩阵运算，即传入第i层的数据构成的向量$h^{i - 1}$乘第i层参数构成的矩阵$W^{i}$，对于首层全连接层$h^{0}$等于训练集${x}$。向量矩阵运算可以被拆分成多个向量乘法运算，也即将全连接层拆分成以神经元为单位的粒度更小的计算节点，不难发现拆分后实现的计算节点间没有数据依赖关系，可以并行地执行。

根据链式求导法则，全连接层反向传播的过程也是矩阵乘法，因此同样可以用正向传播中图节点代表神经元的方式对该层进行数据并行处理。

为了验证COStream对该全连接神经网络模型并行化实现的有效性，选择了IBM-3650服务器作为实验平台进行测试，该服务器是Intel平台的x86-64架构服务器，拥有2个4核的Intel Xeon E5420 CPU，CPU主频为2.50GHz，内存的容量是8GB，最大支持48GB，硬盘容量672GB。实验平台使用的操作系统是Ubuntu16.04，系统的内核版本为4.15.0，编译器所使用的gcc版本为5.4.0，g++版本为5.4.0。经测试如图\ref{fig:dnn_speedup}，根据上述方法实现的全连接网络在不同CPU核数下均表现出良好的并行加速效果。

\begin{figure}[!t]
\centering
\includegraphics[width=0.8\textwidth]{../img/Chap_Application/Yu/dnn_speedup.png}
\caption{示例全连接神经网络多核加速比}
\label{fig:dnn_speedup}
\end{figure}

\subsubsection{LeNet-5 卷积神经网络的并行化分析与设计}

卷积神经网络是一种专门用来处理具有类似网格结构的数据的神经网络，例如图像数据（可以看作二维的像素网格），在卷积网络中至少有一层使用了卷积运算来替代上述全连接层中矩阵乘法运算的神经网络。

一个典型的卷积网络中通常包含三种简单层，在第一层卷积层，并行地计算多个卷积产生一组线性激活响应。在第二层探测层，每一个线性激活响应将会通过一个非线性的激活函数。在第三层池化层中，调用池化函数，它使用某一位置的相邻输出的总体统计特征来替代网络在该位置的输出。值得注意的是，只有卷积层拥有参数。

基于MNIST数据集设计如图\ref{fig:cnn}所示的卷积神经网络解决手写数字识别问题。

\begin{figure}[!t]
\centering
\includegraphics[width=0.8\textwidth]{../img/Chap_Application/Yu/cnn.png}
\caption{卷积神经网络}
\label{fig:cnn}
\end{figure}

对于卷积层，首先每一层通常包含多个卷积核，由于每个卷积核对应的卷积操作是相互独立的，因此可以卷积层按照卷积核数拆分成多个卷积操作，其次，每个卷积核对应多通道，通道间的卷及操作同样是相互独立的，因此可以继续将上述卷积操作继续拆分成对输入的每个通道的卷积操作$conv\_kernel$。此时，卷积层的计算由多个更细粒度的卷积操作构成，可以利用splitjoin结构按照如图\ref{fig:cnn_conv_layer}来实现，先将输入复制分发给每个卷积核，再将输入按照通道逐一分发给对应通道的卷积核。
\begin{figure}[!t]
\centering
\includegraphics[width=0.8\textwidth]{../img/Chap_Application/Yu/cnn_conv_layer.png}
\caption{卷积层模块中composite间的结构关系示意图}
\label{fig:cnn_conv_layer}
\end{figure}

对于池化层，将输入按照不同通道做池化操作，同样利用splitjoin结构来实现，如图\ref{fig:cnn_pooling_layer}。

\begin{figure}[!t]
\centering
\includegraphics[width=0.8\textwidth]{../img/Chap_Application/Yu/cnn_pooling_layer.png}
\caption{池化层模块中composite间的结构关系示意图}
\label{fig:cnn_pooling_layer}
\end{figure}

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.8\textwidth]{../img/Chap_Application/Yu/cnn_speedup.png}
  \caption{示例卷积神经网络多核加速比}
  \label{fig:cnn_speedup}
\end{figure}

为了验证COStream对该卷积神经网络模型并行化实现的有效性，选择了IBM-3650服务器作为实验平台进行测试，该服务器是Intel平台的x86-64架构服务器，拥有2个4核的Intel Xeon E5420 CPU，CPU主频为2.50GHz，内存的容量是8GB，最大支持48GB，硬盘容量672GB。实验平台使用的操作系统是Ubuntu16.04，系统的内核版本为4.15.0，编译器所使用的gcc版本为5.4.0，g++版本为5.4.0。经测试如图\ref{fig:cnn_speedup}，根据上述方法实现的卷积网络在不同CPU核数下均表现出一定的并行加速效果。


\subsection{相关工作}

\subsection{参考文献}
\bibliography{Biblio/Yu.bib}% bibliography